{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc562e32-948a-4cb7-9cef-2b25456a6db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.1967 - loss: 2.5140 - val_accuracy: 0.3758 - val_loss: 1.7536\n",
      "Epoch 2/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.3624 - loss: 1.8157 - val_accuracy: 0.4526 - val_loss: 1.4882\n",
      "Epoch 3/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4298 - loss: 1.6129 - val_accuracy: 0.4960 - val_loss: 1.3788\n",
      "Epoch 4/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.4668 - loss: 1.5023 - val_accuracy: 0.5214 - val_loss: 1.3124\n",
      "Epoch 5/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5017 - loss: 1.3862 - val_accuracy: 0.5374 - val_loss: 1.2644\n",
      "Epoch 6/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5068 - loss: 1.3788 - val_accuracy: 0.5487 - val_loss: 1.2327\n",
      "Epoch 7/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5585 - loss: 1.2762 - val_accuracy: 0.5841 - val_loss: 1.1814\n",
      "Epoch 8/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5625 - loss: 1.2286 - val_accuracy: 0.5794 - val_loss: 1.1528\n",
      "Epoch 9/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5798 - loss: 1.1876 - val_accuracy: 0.5975 - val_loss: 1.1304\n",
      "Epoch 10/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5859 - loss: 1.1692 - val_accuracy: 0.5955 - val_loss: 1.1033\n",
      "Epoch 11/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.5846 - loss: 1.1453 - val_accuracy: 0.6142 - val_loss: 1.0723\n",
      "Epoch 12/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5976 - loss: 1.1305 - val_accuracy: 0.6121 - val_loss: 1.0603\n",
      "Epoch 13/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6325 - loss: 1.0688 - val_accuracy: 0.6121 - val_loss: 1.0399\n",
      "Epoch 14/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6311 - loss: 1.0509 - val_accuracy: 0.6142 - val_loss: 1.0370\n",
      "Epoch 15/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6489 - loss: 1.0075 - val_accuracy: 0.6322 - val_loss: 1.0125\n",
      "Epoch 16/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6600 - loss: 0.9639 - val_accuracy: 0.6415 - val_loss: 1.0084\n",
      "Epoch 17/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6603 - loss: 0.9659 - val_accuracy: 0.6515 - val_loss: 0.9627\n",
      "Epoch 18/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6723 - loss: 0.9254 - val_accuracy: 0.6676 - val_loss: 0.9533\n",
      "Epoch 19/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6794 - loss: 0.9054 - val_accuracy: 0.6702 - val_loss: 0.9486\n",
      "Epoch 20/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6889 - loss: 0.8997 - val_accuracy: 0.6736 - val_loss: 0.9282\n",
      "Epoch 21/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7006 - loss: 0.8624 - val_accuracy: 0.6702 - val_loss: 0.9366\n",
      "Epoch 22/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6959 - loss: 0.8639 - val_accuracy: 0.6769 - val_loss: 0.9103\n",
      "Epoch 23/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6933 - loss: 0.8638 - val_accuracy: 0.6769 - val_loss: 0.9078\n",
      "Epoch 24/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7094 - loss: 0.8340 - val_accuracy: 0.6822 - val_loss: 0.9043\n",
      "Epoch 25/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7008 - loss: 0.8418 - val_accuracy: 0.6782 - val_loss: 0.9149\n",
      "Epoch 26/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7186 - loss: 0.8118 - val_accuracy: 0.6816 - val_loss: 0.9113\n",
      "Epoch 27/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7240 - loss: 0.7918 - val_accuracy: 0.6869 - val_loss: 0.8842\n",
      "Epoch 28/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7215 - loss: 0.7901 - val_accuracy: 0.7023 - val_loss: 0.8691\n",
      "Epoch 29/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7360 - loss: 0.7509 - val_accuracy: 0.7009 - val_loss: 0.8603\n",
      "Epoch 30/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7461 - loss: 0.7258 - val_accuracy: 0.7049 - val_loss: 0.8563\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7065 - loss: 0.8275\n",
      "Accuracy on test set is: 0.7108530402183533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Expected index: 5, Predicted index: [5]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATASET_PATH = \"data_10.json\"\n",
    "\n",
    "# Breaks the data into training and testing sets\n",
    "def load_data(dataset_path):\n",
    "    # Open the file\n",
    "    with open(dataset_path, \"r\") as fp: \n",
    "        data = json.load(fp)\n",
    "\n",
    "    # Convert lists into numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Prepares the dataset for validation/training/testing\n",
    "def prepare_datasets(test_size, validation_size):\n",
    "\n",
    "    # load data\n",
    "    X, y = load_data(DATASET_PATH)\n",
    "    \n",
    "    # create the train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    # create train/validation split\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    # Tensorflow expects a 3D array for each sample\n",
    "    # ex.) (130,13,1) --> 1 is the channel for depth\n",
    "    X_train = X_train[..., np.newaxis] # 4d array from this function -> (num_samples, 130, 13, 1)\n",
    "    X_validation = X_validation[..., np.newaxis] \n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "# Builds the model\n",
    "def build_model(input_shape):\n",
    "\n",
    "    # create the model \n",
    "    model = keras.Sequential()\n",
    "    # 1st conv layer\n",
    "    # Params of add layer:\n",
    "    # 1.) How many Kernels\n",
    "    # 2.) Grid Size of kernels\n",
    "    # 3.) Activation function\n",
    "    # 4.) Input shape = the (130, 13, 1)\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization()) # standardizes the activations in current layer, speeds up training\n",
    "    \n",
    "    # 2nd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((2, 2), strides=(2,2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # flatten the output and feed it into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    # output layer that uses soft max\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to predict on a random sample\n",
    "def predict(model, X, y):\n",
    "\n",
    "    X = X[np.newaxis, ...]\n",
    "    # Needs a 4 D array \n",
    "    # The \"1\" specifies = the number of predictions to make \n",
    "    prediction = model.predict(X) # X --> (1, 130, 13, 1)\n",
    "\n",
    "    # The prediction is a 2D array [[0.1, 0.2, ...]]\n",
    "    # The array will have 10 values for the prediction on each genre for the given sample\n",
    "\n",
    "    # Extract index with max value\n",
    "    predicted_index = np.argmax(prediction, axis=1) # [4] --> the index mapped to a genre label\n",
    "\n",
    "    print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
    "    \n",
    "\n",
    "# Steps:\n",
    "if __name__ == \"__main__\":\n",
    "    # create tain, validation, and test sets \n",
    "\n",
    "    # Test set is after all the training and parameter tweaking\n",
    "    # Validation set --> evaluating our model while we tweak the hyperparameters\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
    "    \n",
    "\n",
    "    # build the CNN net\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]) # taking the dimensions (130, 13, 1)\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # compile the network \n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "    model.compile(optimizer = optimizer, loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # train the CNN \n",
    "    model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
    "    \n",
    "    # evaluate the CNN on the test set\n",
    "    test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
    "    \n",
    "    # Make prediction on a sample\n",
    "    X = X_test[100]\n",
    "    y = y_test[100]\n",
    "    \n",
    "    predict(model, X, y)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
